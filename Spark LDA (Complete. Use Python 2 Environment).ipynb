{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/spark\")\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc =SparkContext()\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic0:\n",
      "vigili  0.00713896446364\n",
      "due  0.00708285329651\n",
      "riesce  0.00608226977368\n",
      "vigile  0.00602941855362\n",
      "arrestato  0.00600086233067\n",
      "pistola  0.00595320275603\n",
      "stato  0.00592130229032\n",
      "strada  0.00588526584937\n",
      "mountainbike  0.00584885973868\n",
      "Firenze,  0.00580306485212\n",
      "\n",
      "\n",
      "Topic1:\n",
      "strada  0.00650637475481\n",
      "zona  0.0062763044834\n",
      "omicidio.  0.00605221657151\n",
      "alcuni  0.00599252848737\n",
      "senza  0.00592982148909\n",
      "finiti  0.00586466396878\n",
      "accorge  0.00584910359443\n",
      "dell'agente  0.00584843074309\n",
      "uscire  0.0057550893171\n",
      "Roberto  0.00574618949919\n",
      "\n",
      "\n",
      "Topic2:\n",
      "consentito  0.00670203116557\n",
      "accaduto  0.00657406005837\n",
      "dimora  0.00619547189446\n",
      "soggiorno,  0.00606321292384\n",
      "arrivano  0.00600156500634\n",
      "magro.  0.00594173633438\n",
      "municipale  0.00591620229118\n",
      "agente  0.00590408111088\n",
      "secondo  0.00590025595787\n",
      "ragazzo  0.00579895070113\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "path = \"/opt/spark/data/mllib/attempt.txt\"\n",
    "italian_stop_words = ['ad', 'al', 'allo', 'ai', 'agli', 'all', 'agl', 'alla', 'alle', 'con', 'col', 'coi', 'da', \n",
    "                      'dal', 'dallo', 'dai', 'dagli', 'dall', 'dagl', 'dalla', 'dalle', 'di', 'del', 'dello', 'dei', \n",
    "                      'degli', 'dell', 'degl', 'della', 'delle', 'in', 'nel', 'nello', 'nei', 'negli', 'nell', 'negl', \n",
    "                      'nella', 'nelle', 's', 'sul', 'sullo', 'sui', 'sugli', 'sull', 'sugl', 'sulla', 'sulle', 'per', \n",
    "                      'tra', 'contro', 'io', 't', 'lui', 'lei', 'noi', 'voi', 'loro', 'mio', 'mia', 'miei', 'mie', \n",
    "                      'tuo', 'tua', 'tuoi', 'tue', 'suo', 'sua', 'suoi', 'sue', 'nostro', 'nostra', 'nostri', 'nostre',\n",
    "                      'vostro', 'vostra', 'vostri', 'vostre', 'mi', 'ti', 'ci', 'vi', 'lo', 'la', 'li', 'le', 'gli', 'ne', \n",
    "                      'il', 'un', 'uno', 'una', 'ma', 'ed', 'se', 'perché', 'anche', 'come', 'dov', 'dove', 'che', 'chi', \n",
    "                      'cui', 'non', 'più', 'quale', 'quanto', 'quanti', 'quanta', 'quante', 'quello', 'quelli', 'quella', \n",
    "                      'quelle', 'questo', 'questi', 'questa', 'queste', 'si', 'tutto', 'tutti', 'a', 'c', 'e', 'i', 'l', \n",
    "                      'o', 'ho', 'hai', 'ha', 'abbiamo', 'avete', 'hanno', 'abbia', 'abbiate', 'abbiano', 'avrà', 'avrai',\n",
    "                      'avrò', 'avremo', 'avrete', 'avranno', 'avrei', 'avresti', 'avrebbe', 'avremmo', 'avreste', 'avrebbero',\n",
    "                      'avevo', 'avevi', 'aveva', 'avevamo', 'avevate', 'avevano', 'ebbi', 'avesti', 'ebbe', 'avemmo', 'aveste',\n",
    "                      'ebbero', 'avessi', 'avesse', 'avessimo', 'avessero', 'avendo', 'avuto', 'avuta', 'avuti', 'avute', 'sono', \n",
    "                      'sei', 'è', 'siamo', 'siete', 'sia', 'siate', 'siano', 'sarà', 'sarai', 'sarò', 'saremo', 'sarete', \n",
    "                      'saranno', 'sarei', 'saresti', 'sarebbe', 'saremmo', 'sareste', 'sarebbero', 'ero', 'eri', 'era', \n",
    "                      'eravamo', 'eravate', 'erano', 'fui', 'fosti', 'fu', 'fummo', 'foste', 'furono', 'fossi', 'fosse', \n",
    "                      'fossimo', 'fossero', 'essendo', 'faccio', 'fai', 'facciamo', 'fanno', 'faccia', 'facciate', 'facciano', \n",
    "                      'farà', 'farai', 'farò', 'faremo', 'farete', 'faranno', 'farei', 'faresti', 'farebbe', 'faremmo', 'fareste', \n",
    "                      'farebbero', 'facevo', 'facevi', 'faceva', 'facevamo', 'facevate', 'facevano', 'feci', 'facesti', 'fece', \n",
    "                      'facemmo', 'faceste', 'fecero', 'facessi', 'facesse', 'facessimo', 'facessero', 'facendo', 'sto', 'stai', \n",
    "                      'sta', 'stiamo', 'stanno', 'stia', 'stiate', 'stiano', 'starà', 'starai', 'starò', 'staremo', 'starete',\n",
    "                      'staranno', 'starei', 'staresti', 'starebbe', 'staremmo', 'stareste', 'starebbero', 'stavo', 'stavi', 'stava', \n",
    "                      'stavamo', 'stavate', 'stavano', 'stetti', 'stesti', 'stette', 'stemmo', 'steste', 'stettero', 'stessi', 'stesse', \n",
    "                      'stessimo', 'stessero', 'stando', \"e'\"]\n",
    "\n",
    "data = sc.textFile(path).zipWithIndex().map(lambda (words,idd): Row(idd= idd, words = words.split(\" \")))\n",
    "docDF = spark.createDataFrame(data)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"clean_words\", stopWords = italian_stop_words)\n",
    "docDF = remover.transform(docDF)\n",
    "\n",
    "Vector = CountVectorizer(inputCol=\"clean_words\", outputCol=\"vectors\")\n",
    "model = Vector.fit(docDF)\n",
    "result = model.transform(docDF)\n",
    "\n",
    "corpus = result.select(\"idd\", \"vectors\").rdd.map(lambda (x,y): [x,Vectors.fromML(y)]).cache()\n",
    "\n",
    "# Cluster the documents into three topics using LDA\n",
    "ldaModel = LDA.train(corpus, k=3,maxIterations=100,optimizer='online')\n",
    "topics = ldaModel.topicsMatrix()\n",
    "vocabArray = model.vocabulary\n",
    "\n",
    "weight = ldaModel.describeTopics(maxTermsPerTopic = 10)\n",
    "\n",
    "wordNumbers = 10  # number of words per topic\n",
    "topicIndices = sc.parallelize(ldaModel.describeTopics(maxTermsPerTopic = wordNumbers))\n",
    "\n",
    "def topic_render(topic):  # specify vector id of words to actual words\n",
    "    terms = topic[0]\n",
    "    result = []\n",
    "    for i in range(wordNumbers):\n",
    "        term = vocabArray[terms[i]]\n",
    "        result.append(term)\n",
    "    return result\n",
    "\n",
    "topics_final = topicIndices.map(lambda topic: topic_render(topic)).collect()\n",
    "\n",
    "#print(vocabArray(int(weight[0][0][0])))\n",
    "i = 0 \n",
    "for topic in range(len(topics_final)):\n",
    "    print (\"Topic\" + str(topic) + \":\")\n",
    "    j = 0\n",
    "    for term in topics_final[topic]:\n",
    "        print (term + '  ' + str(weight[i][1][j]))\n",
    "        j += 1\n",
    "    i += 1\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
