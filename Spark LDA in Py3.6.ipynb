{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/spark\")\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc =SparkContext()\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      " id objectid fa af titl scuol manc prof matemat vuot circ mil cattedr text emergt insegn matemat scuol italian aver fatt cont utilizz dat minister istruzion lanc allarm poch prof matemat quind cattedr ruol rimang vuot problem avvert soprattutt scuol med quest anno cre voragin post rimast vacant dop trasfer soprattutt nord lombard sol arriv par nemmen prossim immission ruol colm vuot vincitor concors sufficient graduator esaur class matemat scienz secondar prim grad già svuot temp concors orma quas complet tutt ital dispon nomin circ professor iscritt graduator numer insufficient ricopr quas quattromil post liber spieg oltre post assegn quind necessar ricorr supplent mancanz alcun professional scuol frutt svil anni valor docenz dov scars attenzion stat confront sistem istruzion termin invest capac vision cos ministr istruzion valer fedel problem poch laur matemat merc lavor soprattutt mond ricerc sottra matemat scuol second indagin almalaure quart laur prosegu dottor univers dev far caric problem laureiam abbast matemat cors laure molt cas ancor figur rifer insegn matemat scuol riform gentil ved insegn sottoprodott matemat consegut svilupp reclut risors sufficient lavor formazion cos giorg bolond ordinar matemat liber univers bolz newspaper fanpag resum dop recent trasfer rimast vacant ben post class concors insegn matemat scienz second elabor tuttoscuol dat miur quas que post trov region settentrional author susann picon dat lugl condivision np tags np link http www fanpag it scuol manc prof matemat vuot circ mil cattedr\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = SnowballStemmer(\"italian\") \n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('italian'))\n",
    "\n",
    "file_path = '/root/Desktop/News'\n",
    "\n",
    "\n",
    "# Routine for cleaning documents\n",
    "# ---------------------------------\n",
    "def clean(path):\n",
    "    with  open(path, 'r') as to_clean:\n",
    "        one_line = ''\n",
    "        for string in to_clean.readlines():\n",
    "            one_line += string.lower()\n",
    "    to_clean.close()\n",
    "    \n",
    "    #Special espressions of html format\n",
    "    to_replace = ['\\\\n','\\\\t','\\\\r', '\\\\', '&nbsp'] \n",
    "    \n",
    "    for item in to_replace:\n",
    "        one_line = one_line.replace(item,' ') \n",
    "\n",
    "    cleaned = ' '.join(word for word in one_line.split() if len(word)>1)\n",
    "    \n",
    "    # All other special characters\n",
    "    definitive = re.sub('[^a-zA-Zàéòùè]', ' ', cleaned)\n",
    "    \n",
    "    # Just the stopwords remain\n",
    "    word_tokens = word_tokenize(definitive)\n",
    " \n",
    "    filtered_text = [w for w in word_tokens if not w in stop_words]\n",
    "    cleaned = ' '.join(word for word in filtered_text if len(word)>1)\n",
    "    \n",
    "    with open(path, 'w') as to_clean:\n",
    "        to_clean.write(cleaned) \n",
    "        \n",
    "    to_clean.close()\n",
    "    \n",
    "    return \n",
    "# -----------------------------------------\n",
    "\n",
    "cleaned_text = clean(file_path)\n",
    "\n",
    "full_cleaned_text = str(\"\")\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        full_cleaned_text = full_cleaned_text + \" \" + str(line)\n",
    "        \n",
    "tokens = nltk.word_tokenize(full_cleaned_text)\n",
    "\n",
    "stemmed_text = \"\"\n",
    "\n",
    "for token in tokens:\n",
    "    stemmed_text = stemmed_text + \" \" + stemmer.stem(token)\n",
    "    \n",
    "print(stemmed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic0:\n",
      "matematica  0.009560573221092942\n",
      "scuola  0.00808030602233562\n",
      "posti  0.007174175578550224\n",
      "vuote  0.00701157776472776\n",
      "quasi  0.006984416666260308\n",
      "capacità  0.006773406498557413\n",
      "istruzione  0.0066294229384543345\n",
      "ben  0.006600503209852239\n",
      "aver  0.006556311017571411\n",
      "reclutano  0.0065142697691635085\n",
      "\n",
      "\n",
      "Topic1:\n",
      "frutto  0.00755124839871547\n",
      "voragine  0.007512074134983625\n",
      "lombardia  0.007231254609200753\n",
      "professionalità  0.007015777869361581\n",
      "fedeli  0.006789730066915294\n",
      "date  0.006683804983393134\n",
      "ministra  0.006668940226489626\n",
      "confronti  0.0066030669225653794\n",
      "fa  0.00660271421692034\n",
      "pare  0.006571353930747912\n",
      "\n",
      "\n",
      "Topic2:\n",
      "pare  0.006934124243330362\n",
      "settentrionali  0.006785717924896101\n",
      "primo  0.006783615605131653\n",
      "concorsi  0.0067461302177306\n",
      "tutta  0.006697932883073028\n",
      "svilimento  0.006627104343063499\n",
      "scienze  0.0066031510387970435\n",
      "anni  0.0065838917919853\n",
      "resume  0.006506316001530461\n",
      "link  0.0064634905197354185\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "data = sc.textFile(file_path).zipWithIndex().map( lambda cleanwords_idd: Row(idd = cleanwords_idd[1], cleanwords = cleanwords_idd[0].split(\" \")))\n",
    "docDF = spark.createDataFrame(data)\n",
    "\n",
    "\n",
    "Vector = CountVectorizer(inputCol=\"cleanwords\", outputCol=\"vectors\")\n",
    "model = Vector.fit(docDF)\n",
    "result = model.transform(docDF)\n",
    "\n",
    "corpus = result.select(\"idd\", \"vectors\").rdd.map(lambda x_y: [x_y[0],Vectors.fromML(x_y[1])]).cache()\n",
    "\n",
    "# Cluster the documents into three topics using LDA\n",
    "ldaModel = LDA.train(corpus, k=3,maxIterations=100,optimizer='online')\n",
    "topics = ldaModel.topicsMatrix()\n",
    "vocabArray = model.vocabulary\n",
    "\n",
    "weight = ldaModel.describeTopics(maxTermsPerTopic = 10)\n",
    "\n",
    "wordNumbers = 10  # number of words per topic\n",
    "topicIndices = sc.parallelize(ldaModel.describeTopics(maxTermsPerTopic = wordNumbers))\n",
    "\n",
    "def topic_render(topic):  # specify vector id of words to actual words\n",
    "    terms = topic[0]\n",
    "    result = []\n",
    "    for i in range(wordNumbers):\n",
    "        term = vocabArray[terms[i]]\n",
    "        result.append(term)\n",
    "    return result\n",
    "\n",
    "topics_final = topicIndices.map(lambda topic: topic_render(topic)).collect()\n",
    "\n",
    "#print(vocabArray(int(weight[0][0][0])))\n",
    "i = 0 \n",
    "for topic in range(len(topics_final)):\n",
    "    print (\"Topic\" + str(topic) + \":\")\n",
    "    j = 0\n",
    "    for term in topics_final[topic]:\n",
    "        print (term + '  ' + str(weight[i][1][j]))\n",
    "        j += 1\n",
    "    i += 1\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
